{"cells":[{"metadata":{},"cell_type":"markdown","source":["Source: https://github.com/mattm/simple-neural-network/blob/master/neural-network.py\n","\n","Calculations: https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n","\n","**Important remark**: In this example, all neurons in a layer share the same bias, which is never updated. Fot the sake of simplicity I left it as it was in the original source, BUT typically this is NOT the case. All neurons should have their own bias, which will be updated during backpropagation in the same way as any other weight."]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"],"execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["A class for a perceptron is defined, with the following functions:\n","\n","**Forward step**\n","* calculate_output: returns y = phi(sum(wk * xk) + b)\n","* calculate_total_net_input: helper function for calculate_output, returns (sum(wk * xk) + b)\n","* squash: activation function for the neuron -- this corresponds to phi\n","\n","**Helpers for backpropagation**\n","* calculate_pd_error_wrt_total_net_input\n","* calculate_pd_error_wrt_output\n","* calculate_pd_total_net_input_wrt_input\n","* calculate_pd_total_net_input_wrt_weight\n"]},{"metadata":{"trusted":true},"cell_type":"code","source":["import random\n","import math\n","\n","# A class for a perceptron is defined, with the following functions:\n","\n","# Functions \n","# calculate_output: returns y = phi(sum(wk * xk) + b)\n","# calculate_total_net_input: helper function for calculate_output, returns (sum(wk * xk) + b)\n","# squash: activation function for the neuron -- this corresponds to phi\n","\n","# functions to calculate the partial derivatives\n","\n","class Neuron:\n","    def __init__(self, bias):\n","        self.bias = bias\n","        self.weights = []\n","\n","    def calculate_output(self, inputs):\n","        self.inputs = inputs\n","        self.output = self.squash(self.calculate_total_net_input())\n","        return self.output\n","\n","    def calculate_total_net_input(self):\n","        total = 0\n","        for i in range(len(self.inputs)):\n","            total += self.inputs[i] * self.weights[i]\n","        return total + self.bias\n","\n","    def squash(self, total_net_input):\n","        return 1 / (1 + math.exp(-total_net_input))\n","\n","    # Determine how much the neuron's total input has to change to move closer to the expected output\n","    #\n","    # Now that we have the partial derivative of the error with respect to the output (∂E/∂yⱼ) and\n","    # the derivative of the output with respect to the total net input (dyⱼ/dzⱼ) we can calculate\n","    # the partial derivative of the error with respect to the total net input.\n","    # This value is also known as the delta (δ) [1]\n","    # δ = ∂E/∂zⱼ = ∂E/∂yⱼ * dyⱼ/dzⱼ\n","    #\n","    def calculate_pd_error_wrt_total_net_input(self, target_output):\n","        return self.calculate_pd_error_wrt_output(target_output) * self.calculate_pd_total_net_input_wrt_input();\n","\n","    # The error for each neuron is calculated by the Mean Square Error method:\n","    def calculate_error(self, target_output):\n","        return 0.5 * (target_output - self.output) ** 2\n","\n","    # The partial derivate of the error with respect to actual output then is calculated by:\n","    # = 2 * 0.5 * (target output - actual output) ^ (2 - 1) * -1\n","    # = -(target output - actual output)\n","    #\n","    # The Wikipedia article on backpropagation [1] simplifies to the following, but most other learning material does not [2]\n","    # = actual output - target output\n","    #\n","    # Alternative, you can use (target - output), but then need to add it during backpropagation [3]\n","    #\n","    # Note that the actual output of the output neuron is often written as yⱼ and target output as tⱼ so:\n","    # = ∂E/∂yⱼ = -(tⱼ - yⱼ)\n","    def calculate_pd_error_wrt_output(self, target_output):\n","        return -(target_output - self.output)\n","\n","    # The total net input into the neuron is squashed using logistic function to calculate the neuron's output:\n","    # yⱼ = φ = 1 / (1 + e^(-zⱼ))\n","    # Note that where ⱼ represents the output of the neurons in whatever layer we're looking at and ᵢ represents the layer below it\n","    #\n","    # The derivative (not partial derivative since there is only one variable) of the output then is:\n","    # dyⱼ/dzⱼ = yⱼ * (1 - yⱼ)\n","    def calculate_pd_total_net_input_wrt_input(self):\n","        return self.output * (1 - self.output)\n","\n","    # The total net input is the weighted sum of all the inputs to the neuron and their respective weights:\n","    # = zⱼ = netⱼ = x₁w₁ + x₂w₂ ...\n","    #\n","    # The partial derivative of the total net input with respective to a given weight (with everything else held constant) then is:\n","    # = ∂zⱼ/∂wᵢ = some constant + 1 * xᵢw₁^(1-0) + some constant ... = xᵢ\n","    def calculate_pd_total_net_input_wrt_weight(self, index):\n","        return self.inputs[index]"],"execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["class NeuronLayer:\n","    def __init__(self, num_neurons, bias):\n","\n","        # Every neuron in a layer shares the same bias -- just in this example, usually this is not the case!\n","        self.bias = bias if bias else random.random()\n","\n","        self.neurons = []\n","        for i in range(num_neurons):\n","            self.neurons.append(Neuron(self.bias))\n","\n","    def inspect(self):\n","        print('Neurons:', len(self.neurons))\n","        for n in range(len(self.neurons)):\n","            print(' Neuron', n)\n","            for w in range(len(self.neurons[n].weights)):\n","                print('  Weight:', self.neurons[n].weights[w])\n","            print('  Bias:', self.neurons[n].bias)\n","\n","    def feed_forward(self, inputs):\n","        outputs = []\n","        for neuron in self.neurons:\n","            outputs.append(neuron.calculate_output(inputs))\n","        return outputs\n","\n","    def get_outputs(self):\n","        outputs = []\n","        for neuron in self.neurons:\n","            outputs.append(neuron.output)\n","        return outputs"],"execution_count":44,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":["\n","\n","class NeuralNetwork:\n","    LEARNING_RATE = 0.5\n","\n","    def __init__(self, num_inputs, num_hidden, num_outputs, hidden_layer_weights = None, hidden_layer_bias = None, output_layer_weights = None, output_layer_bias = None):\n","        self.num_inputs = num_inputs\n","\n","        self.hidden_layer = NeuronLayer(num_hidden, hidden_layer_bias)\n","        self.output_layer = NeuronLayer(num_outputs, output_layer_bias)\n","\n","        self.init_weights_from_inputs_to_hidden_layer_neurons(hidden_layer_weights)\n","        self.init_weights_from_hidden_layer_neurons_to_output_layer_neurons(output_layer_weights)\n","\n","    def init_weights_from_inputs_to_hidden_layer_neurons(self, hidden_layer_weights):\n","        weight_num = 0\n","        for h in range(len(self.hidden_layer.neurons)):\n","            for i in range(self.num_inputs):\n","                if not hidden_layer_weights:\n","                    self.hidden_layer.neurons[h].weights.append(random.random())\n","                else:\n","                    self.hidden_layer.neurons[h].weights.append(hidden_layer_weights[weight_num])\n","                weight_num += 1\n","\n","    def init_weights_from_hidden_layer_neurons_to_output_layer_neurons(self, output_layer_weights):\n","        weight_num = 0\n","        for o in range(len(self.output_layer.neurons)):\n","            for h in range(len(self.hidden_layer.neurons)):\n","                if not output_layer_weights:\n","                    self.output_layer.neurons[o].weights.append(random.random())\n","                else:\n","                    self.output_layer.neurons[o].weights.append(output_layer_weights[weight_num])\n","                weight_num += 1\n","\n","    def inspect(self):\n","        print('------')\n","        print('* Inputs: {}'.format(self.num_inputs))\n","        print('------')\n","        print('Hidden Layer')\n","        self.hidden_layer.inspect()\n","        print('------')\n","        print('* Output Layer')\n","        self.output_layer.inspect()\n","        print('------')\n","\n","    def feed_forward(self, inputs):\n","        hidden_layer_outputs = self.hidden_layer.feed_forward(inputs)\n","        return self.output_layer.feed_forward(hidden_layer_outputs)\n","\n","    # Uses online learning, ie updating the weights after each training case\n","    def train(self, training_inputs, training_outputs):\n","        self.feed_forward(training_inputs)\n","\n","        # 1. Output neuron deltas\n","        pd_errors_wrt_output_neuron_total_net_input = [0] * len(self.output_layer.neurons)\n","        for o in range(len(self.output_layer.neurons)):\n","\n","            # ∂E/∂zⱼ\n","            pd_errors_wrt_output_neuron_total_net_input[o] = self.output_layer.neurons[o].calculate_pd_error_wrt_total_net_input(training_outputs[o])\n","\n","        # 2. Hidden neuron deltas\n","        pd_errors_wrt_hidden_neuron_total_net_input = [0] * len(self.hidden_layer.neurons)\n","        for h in range(len(self.hidden_layer.neurons)):\n","\n","            # We need to calculate the derivative of the error with respect to the output of each hidden layer neuron\n","            # dE/dyⱼ = Σ ∂E/∂zⱼ * ∂z/∂yⱼ = Σ ∂E/∂zⱼ * wᵢⱼ\n","            d_error_wrt_hidden_neuron_output = 0\n","            for o in range(len(self.output_layer.neurons)):\n","                d_error_wrt_hidden_neuron_output += pd_errors_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].weights[h]\n","\n","            # ∂E/∂zⱼ = dE/dyⱼ * ∂zⱼ/∂\n","            pd_errors_wrt_hidden_neuron_total_net_input[h] = d_error_wrt_hidden_neuron_output * self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_input()\n","\n","        # 3. Update output neuron weights\n","        for o in range(len(self.output_layer.neurons)):\n","            for w_ho in range(len(self.output_layer.neurons[o].weights)):\n","\n","                # ∂Eⱼ/∂wᵢⱼ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢⱼ\n","                pd_error_wrt_weight = pd_errors_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].calculate_pd_total_net_input_wrt_weight(w_ho)\n","\n","                # Δw = α * ∂Eⱼ/∂wᵢ\n","                self.output_layer.neurons[o].weights[w_ho] -= self.LEARNING_RATE * pd_error_wrt_weight\n","\n","        # 4. Update hidden neuron weights\n","        for h in range(len(self.hidden_layer.neurons)):\n","            for w_ih in range(len(self.hidden_layer.neurons[h].weights)):\n","\n","                # ∂Eⱼ/∂wᵢ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢ\n","                pd_error_wrt_weight = pd_errors_wrt_hidden_neuron_total_net_input[h] * self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_weight(w_ih)\n","\n","                # Δw = α * ∂Eⱼ/∂wᵢ\n","                self.hidden_layer.neurons[h].weights[w_ih] -= self.LEARNING_RATE * pd_error_wrt_weight\n","\n","    def calculate_total_error(self, training_sets):\n","        total_error = 0\n","        for t in range(len(training_sets)):\n","            training_inputs, training_outputs = training_sets[t]\n","            self.feed_forward(training_inputs)\n","            for o in range(len(training_outputs)):\n","                total_error += self.output_layer.neurons[o].calculate_error(training_outputs[o])\n","        return total_error\n","\n","\n","\n"],"execution_count":45,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Calculated example\n","\n","nn = NeuralNetwork(2, 2, 2, hidden_layer_weights=[0.15, 0.2, 0.25, 0.3], hidden_layer_bias=0.35, output_layer_weights=[0.4, 0.45, 0.5, 0.55], output_layer_bias=0.6)\n","\n","nn.inspect()\n","\n","print(' ')\n","print(' ')\n","print('Output of the network for [0.05, 0.1]')\n","print('y = ', nn.feed_forward([0.05, 0.1]))\n","\n","print('Error: ',nn.calculate_total_error([[[0.05, 0.1], [0.01, 0.99]]]))\n","\n","\n","nn.train([0.05, 0.1], [0.01, 0.99])\n","\n","print(' ')\n","print(' ')\n","nn.inspect()\n","\n","print(' ')\n","print(' ')\n","print('Output of the network for [0.05, 0.1]')\n","print('y = ', nn.feed_forward([0.05, 0.1]))\n","print('Error: ',nn.calculate_total_error([[[0.05, 0.1], [0.01, 0.99]]]))"],"execution_count":46,"outputs":[{"output_type":"stream","text":"------\n* Inputs: 2\n------\nHidden Layer\nNeurons: 2\n Neuron 0\n  Weight: 0.15\n  Weight: 0.2\n  Bias: 0.35\n Neuron 1\n  Weight: 0.25\n  Weight: 0.3\n  Bias: 0.35\n------\n* Output Layer\nNeurons: 2\n Neuron 0\n  Weight: 0.4\n  Weight: 0.45\n  Bias: 0.6\n Neuron 1\n  Weight: 0.5\n  Weight: 0.55\n  Bias: 0.6\n------\n \n \nOutput of the network for [0.05, 0.1]\ny =  [0.7513650695523157, 0.7729284653214625]\nError:  0.2983711087600027\n \n \n------\n* Inputs: 2\n------\nHidden Layer\nNeurons: 2\n Neuron 0\n  Weight: 0.1497807161327628\n  Weight: 0.19956143226552567\n  Bias: 0.35\n Neuron 1\n  Weight: 0.24975114363236958\n  Weight: 0.29950228726473915\n  Bias: 0.35\n------\n* Output Layer\nNeurons: 2\n Neuron 0\n  Weight: 0.35891647971788465\n  Weight: 0.4086661860762334\n  Bias: 0.6\n Neuron 1\n  Weight: 0.5113012702387375\n  Weight: 0.5613701211079891\n  Bias: 0.6\n------\ny =  [0.7420881111907824, 0.7752849682944595]\nError:  0.29102777369359933\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6-final","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}